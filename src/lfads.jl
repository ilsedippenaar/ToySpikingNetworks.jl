using Flux
using Flux: @functor, flip
using MLUtils: chunk, DataLoader
using Distributions
import StatsBase: kldivergence

struct BiRNN{T}
    f::T
    r::T
end
@functor BiRNN
BiRNN(T, (in, out)::Pair; kwargs...) = BiRNN(T(in => out; kwargs...), T(in => out; kwargs...))
(m::BiRNN)(xs::AbstractVector) = vcat.([m.f(x) for x in xs], flip(m.r, xs))
(m::BiRNN)(xs::AbstractArray{T,3}) where T = m(eachslice(xs, dims=3) |> collect)

struct BiEncoder{T}
    f::T
    r::T
end
@functor BiEncoder
BiEncoder(T, (in, out)::Pair; kwargs...) = BiEncoder(T(in => out; kwargs...), T(in => out; kwargs...))
(m::BiEncoder)(xs::AbstractVector) = vcat(last(m.f(x) for x in xs), last(m.r(x) for x in Iterators.reverse(xs)))
(m::BiEncoder)(xs::AbstractArray{T,3}) where T  = m(eachslice(xs, dims=3) |> collect)

struct ToDistribution{D} end
function (::ToDistribution{D})(x) where D
    @assert iszero(size(x, 1) % fieldcount(D))
    D.(chunk(x, fieldcount(D), dims=1)...)
end
function (::ToDistribution{Normal})(x)
    @assert iseven(size(x, 1))
    Î¼, logÏƒÂ² = chunk(x, 2, dims=1)
    Normal.(Î¼, exp.(logÏƒÂ²/2))
end

@Base.kwdef struct LFADSArgs
    input_size::Integer
    g_size::Integer = 16
    factor_size::Integer = 4
    u_size::Integer = 2

    g_encoder_size::Integer = 16
    c_encoder_size::Integer = 16
    generator_size::Integer = 16
    controller_size::Integer = 16
    dropout::Float32 = 0.1f0
end

struct LFADSGeneratorCell
    Wáµ
    controller
    RNNáµáµ‰â¿_cell
    Wá¶ 
    WÊ³
end
@functor LFADSGeneratorCell
function LFADSGeneratorCell(a::LFADSArgs)
    # for clarity
    args = (
        Wáµ = a.g_size == a.generator_size ? identity : Dense(a.g_size => a.generator_size),
        controller = Chain(
            GRU(2a.c_encoder_size + a.factor_size => a.controller_size),  # RNNá¶œáµ’â¿
            Dropout(a.dropout),
            Dense(a.controller_size => 2a.u_size),  # Wáµ˜
            ToDistribution{Normal}(),
        ),
        RNNáµáµ‰â¿_cell = Flux.GRUCell(a.u_size => a.generator_size),
        Wá¶  = Chain(Dropout(a.dropout), Dense(a.generator_size => a.factor_size)),
        WÊ³ = Dense(a.factor_size => a.input_size),
    )
    LFADSGeneratorCell(args...)
end
function Flux.Recur(m::LFADSGeneratorCell, gÌ‚â‚€)
    gÌ‚â‚€ = m.Wáµ(gÌ‚â‚€)
    RNNáµáµ‰â¿ = Flux.Recur(m.RNNáµáµ‰â¿_cell, gÌ‚â‚€)
    fâ‚€ = m.Wá¶ (gÌ‚â‚€)
    Flux.Recur(fâ‚€) do fâ‚œâ‚‹â‚, Eá¶œáµ’â¿â‚œ
        uâ‚œ = m.controller([Eá¶œáµ’â¿â‚œ; fâ‚œâ‚‹â‚])
        uÌ‚â‚œ = rand.(uâ‚œ)  # TODO: do we need the reparameterization trick?
        gâ‚œ = RNNáµáµ‰â¿(uÌ‚â‚œ)
        fâ‚œ = m.Wá¶ (gâ‚œ)
        râ‚œ = Poisson.(exp.(m.WÊ³(fâ‚œ)))
        fâ‚œ, (;uâ‚œ, râ‚œ)
    end
end

struct LFADSModel
    g_encoder
    c_encoder
    generator_cell
end
@functor LFADSModel
LFADSModel(a::LFADSArgs) = LFADSModel(
    Chain(
        BiEncoder(GRU, a.input_size => a.g_encoder_size),
        Dropout(a.dropout),
        Dense(2a.g_encoder_size => 2a.g_size),
        ToDistribution{Normal}(),
    ),
    BiRNN(GRU, a.input_size => a.c_encoder_size),
    LFADSGeneratorCell(a),
)
function (m::LFADSModel)(xs)
    gâ‚€ = m.g_encoder(xs)
    gÌ‚â‚€ = rand.(gâ‚€)
    Eá¶œáµ’â¿ = m.c_encoder(xs)
    gen = Flux.Recur(m.generator_cell, gÌ‚â‚€)
    output = [gen(Eá¶œáµ’â¿â‚œ) for Eá¶œáµ’â¿â‚œ in Eá¶œáµ’â¿]
    (;gâ‚€, u = getfield.(output, :uâ‚œ), r = getfield.(output, :râ‚œ))
end

struct NormalAR1{T<:Real} <: Distribution{Univariate, Continuous}
    Î¼::Vector{T}
    logÏƒÂ²â‚š::Vector{T}
    logÏ„::Vector{T}
end
NormalAR1(Î¼::Real, logÏƒÂ²::Real, logÏ„::Real) = NormalAR1(([x] for x in promote(Î¼, logÏƒÂ², logÏ„))...)
@functor NormalAR1
function kldivergence(P_Î¸x::AbstractVector{Normal{T}}, P_Î¸::NormalAR1{T}) where T
    # TODO: numerical stability?
    Î¼, logÏƒÂ²â‚š, logÏ„ = P_Î¸.Î¼[1], P_Î¸.logÏƒÂ²â‚š[1], P_Î¸.logÏ„[1]
    Ï„ = exp(logÏ„)
    ÏƒÂ²â‚š = exp(logÏƒÂ²â‚š)
    Î± = exp(-1/Ï„)
    ÏƒÂ²â‚‘ = ÏƒÂ²â‚š*(1-Î±^2)
    kldivergence(P_Î¸x[1], Normal(Î¼, âˆšÏƒÂ²â‚‘)) + sum(
        kldivergence.(P_Î¸x[2:end], @. convolve(Î¼ + Î±*(P_Î¸x[1:end-1] - Î¼), Normal(0, âˆšÏƒÂ²â‚‘)))
    )
end

struct LearnableNormal{T<:Real} <: Distribution{Univariate, Continuous}
    Î¼::Vector{T}
    logÏƒÂ²::Vector{T}
end
@functor LearnableNormal
LearnableNormal(Î¼::Real, logÏƒÂ²::Real) = (d = Normal(Î¼, âˆšexp(logÏƒÂ²)); LearnableNormal{eltype(d)}([d.Î¼], [log(var(d))]))
Normal(p::LearnableNormal) = Normal(p.Î¼[1], p.logÏƒÂ²[1] |> exp |> sqrt)
kldivergence(p::Normal, q::LearnableNormal) = kldivergence(p, Normal(q))

struct LFADS
    model
    Páµ
    Páµ˜
end
@functor LFADS
function loss(lfads::LFADS, input::AbstractVector)  # input = time x [features x batch]
    gâ‚€, u, r = lfads.model(input)
    input, u, r = cat(input..., dims=3), cat(u..., dims=3), cat(r..., dims=3)
    ğ“›Ë£ = mean(sum(logpdf.(r, input), dims=(1, 3)))
    ğ“›á´°á´¸ = mean(sum(kldivergence.(gâ‚€, lfads.Páµ), dims=1)) + mean(sum(mapslices(u -> kldivergence(u, lfads.Páµ˜), u, dims=3), dims=1))
    -(ğ“›Ë£ - ğ“›á´°á´¸)
end


rates = [4, 5, 2, 5, 8, 7, 6, 3, 2, 1]
X = cat([rand(Poisson(r), 1, 4, 100) .|> Float32 for r in rates]..., dims=1)
X = eachslice(X, dims=3) |> collect

args = LFADSArgs(input_size=length(rates))
model = LFADSModel(args)
lfads = LFADS(model, LearnableNormal(0f0, 0f0), NormalAR1(0f0, -1f0, -1f0))

train_set = [X, X, X]

# gâ‚€, u, r = lfads.model(X)

# TODO: Adam params
# TODO: training params (num_epoch)
# TODO: loss weight parameters
opt_state = Flux.setup(Adam(), lfads)
my_log = []
for epoch in 1:100
  losses = Float32[]
  Flux.trainmode!(lfads)
  for (i, data) in enumerate(train_set)
    ğ“›, âˆ‡ = Flux.withgradient(m->loss(m, data), lfads)
    push!(losses, ğ“›)
    if !isfinite(ğ“›)
      @warn "loss is $ğ“› on item $i" epoch
    else
        Flux.update!(opt_state, lfads, âˆ‡[1])
    end
    Flux.reset!(lfads)
  end...
  Flux.testmode!(lfads)
  acc = mean(abs.(rates .- mean(
    getfield.(cat(lfads.model(X).r..., dims=3)[:, 4, :], :Î»),
    dims=2)))
  push!(my_log, (; acc, losses))
end

my_log

using CairoMakie
lines(getfield.(my_log, :acc))
lines()
lines(vcat(getfield.(my_log, :losses)...))
